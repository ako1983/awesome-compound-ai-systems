{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ako1983/awesome-compound-ai-systems/blob/main/Friendli_GIFpt_Challenge_1112.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x5ocVybjn_5"
      },
      "source": [
        "# **Friendli GIFpt Challenge**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5hGePgLbbDU"
      },
      "source": [
        "# **Friendli GIF chatbot**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhsWqk2Obgl2"
      },
      "source": [
        "## ***Getting Started***\n",
        "\n",
        "\n",
        "1.   Head to [*https://suite.friendli.ai*](https://suite.friendli.ai/signup/self-register?product=serverless-endpoints), and create an account.\n",
        "2.   Grab a [**`FRIENDLI_TOKEN`**](https://suite.friendli.ai/user-settings/tokens) to use Friendli Serverless Endpoints for the assistant.\n",
        "3.   Install dependencies\n",
        "\n",
        "API KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqSo2lONNTlF",
        "outputId": "edcd2671-70db-4406-cfb4-9263d5c80c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install openai gradio requests --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inGwDMsncPeW"
      },
      "source": [
        "### 🚀 *Build and visualize your assistant!*\n",
        "1. Do the function call using meta-llama-3.1-70b-instruct\n",
        "2. Pass the output image as an input of llama 3.2 vision model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06yIqRogD8yw"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import gradio as gr\n",
        "import json\n",
        "from getpass import getpass\n",
        "from openai import OpenAI\n",
        "\n",
        "css = \"\"\"\n",
        ".gradio-container {\n",
        "    max-width: 800px !important;\n",
        "    margin-top: 100px !important;\n",
        "}\n",
        ".pending {\n",
        "    display: none !important;\n",
        "}\n",
        ".sm {\n",
        "    box-shadow: None !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "GIFPY_API_KEY = \"AZlZzMlDoD27PLV9QPJ5GvU7UvpuLgHt\"\n",
        "FRIENDLI_TOKEN = getpass(\"FRIENDLI_TOKEN:\")\n",
        "\n",
        "friendli_client = OpenAI(\n",
        "    base_url=\"https://inference.friendli.ai/v1\",\n",
        "    api_key=FRIENDLI_TOKEN\n",
        ")\n",
        "\n",
        "friendli_beta_client = OpenAI(\n",
        "    base_url=\"https://inference.friendli.ai/beta\",\n",
        "    api_key=FRIENDLI_TOKEN\n",
        ")\n",
        "\n",
        "def search_gif(keyword):\n",
        "    response = requests.get(f\"https://api.giphy.com/v1/gifs/search?api_key={GIFPY_API_KEY}&q={keyword}&limit=1&offset=0&rating=g&lang=en\")\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if data['data']:\n",
        "            return data['data'][0]['images']['original']['url']\n",
        "\n",
        "    return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "DkxQDS7oc8Q9",
        "outputId": "1f484805-51dd-48dc-bc23-21aa7ad7e23c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a5458be3eb1d27a4ed.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a5458be3eb1d27a4ed.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "def chatbot_response(message, history):\n",
        "    # message -> extract keyword for a GIF using llm\n",
        "    tools = [{\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"search_gif\",\n",
        "            \"description\": \"Search for a GIF using the given keyword.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"keyword\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"A keyword for a GIF that fits in the conversation.\",\n",
        "                    },\n",
        "                },\n",
        "            },\n",
        "        },\n",
        "    }]\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": \"Find a keyword for a GIF that fits in the conversation.\"}]\n",
        "    for user, chatbot in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": chatbot})\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    tool_response = friendli_client.chat.completions.create(\n",
        "        model=\"meta-llama-3.1-70b-instruct\",  # Fill in the model name here\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "    )\n",
        "    keyword = message\n",
        "    if tool_response.choices[0].message.tool_calls:\n",
        "        tool_call = tool_response.choices[0].message.tool_calls[0]\n",
        "        arguments = json.loads(tool_call.function.arguments)\n",
        "        keyword = arguments.get('keyword')\n",
        "\n",
        "    gif_url = search_gif(keyword)\n",
        "    messages_for_response = []\n",
        "    if gif_url:\n",
        "        # messages_for_response.append({\n",
        "        #     \"role\": \"system\",\n",
        "        #     \"content\": \"The input image is actually what you will answer with, answer properly.\"\n",
        "        # })\n",
        "        for user, chatbot in history:\n",
        "            messages_for_response.append({\"role\": \"user\", \"content\": user})\n",
        "            messages_for_response.append({\"role\": \"assistant\", \"content\": chatbot})\n",
        "        messages_for_response.append({\n",
        "            \"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"text\", \"text\": message + \"\\n\\nThe input image is actually what you will answer with, answer properly.\"},\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": gif_url}}\n",
        "            ]\n",
        "        })\n",
        "    else:\n",
        "        for user, chatbot in history:\n",
        "            messages_for_response.append({\"role\": \"user\", \"content\": user})\n",
        "            messages_for_response.append({\"role\": \"assistant\", \"content\": chatbot})\n",
        "        messages_for_response.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    reply_response = friendli_beta_client.chat.completions.create(\n",
        "        model=\"llama-3.2-11b-vision-instruct\",  # Fill in the model name here too\n",
        "        messages=messages_for_response,  # Fill in the variable for messages here\n",
        "        timeout=60,\n",
        "        stream=True,\n",
        "    )\n",
        "    reply = \"\"\n",
        "    for chunk in reply_response:\n",
        "        if chunk.choices is not None:\n",
        "            reply += chunk.choices[0].delta.content or \"\"\n",
        "    return reply, gif_url\n",
        "\n",
        "\n",
        "\n",
        "def chat_interface_fn(message, history):\n",
        "    reply, gif = chatbot_response(message, history)\n",
        "    if gif:\n",
        "        return [(message, f\"{reply}\\n![GIF]({gif})\")]\n",
        "    return [(message, f\"{reply}\")]\n",
        "\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(), css=css) as agent:\n",
        "    def respond(message, history: list):\n",
        "        response = chat_interface_fn(message, history)\n",
        "        history.extend(response)\n",
        "        return history, history\n",
        "\n",
        "    chat = gr.Chatbot()\n",
        "    textbox = gr.Textbox(show_label=False, placeholder=\"Type a message...\")\n",
        "    textbox.submit(respond, [textbox, chat], [chat, chat])\n",
        "\n",
        "agent.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}